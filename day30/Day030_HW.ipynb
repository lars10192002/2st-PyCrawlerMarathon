{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實務上的爬蟲與挑戰\n",
    "\n",
    "\n",
    "* 實務上爬蟲可能遇到的問題有哪些\n",
    "* 淺談常見防爬蟲機制與處理策略\n",
    "* 如何建構一個可以自動持續更新的爬蟲程式\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "* （簡答題）試著舉出一到三個爬蟲可能會遇到的問題導致無法抓取的？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### （簡答題）試著舉出一到三個爬蟲可能會遇到的問題導致無法抓取的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYour Input Here\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "https://blog.csdn.net/weixin_41179709/article/details/83278877\n",
    "網站的反爬蟲機制：\n",
    "1. 偽裝瀏覽器\n",
    "由於爬蟲多直接由python腳本直接訪問網頁，部分企業也就可以通過建立建立識別來訪者是否為Python腳本訪問，所以，我們可以使用偽裝瀏覽器的方式進行防禦方式進行破解。\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib import  request\n",
    "\n",
    "url = \"http://www.cbrc.gov.cn/chinese/jrjg/index.html\"\n",
    "# 如何伪装成浏览器访问?\n",
    "# 1. 定义一个真实浏览器的代理名称\n",
    "user_agent = \"Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101 Firefox/45.0\"\n",
    "# 2. 写到请求页面的header里面去\n",
    "req = request.Request(url,headers={'User-Agent': user_agent} )\n",
    "#  3. 打开网页， 获取内容\n",
    "print(urlopen(req).read().decode('utf-8'))\n",
    "\n",
    "[檢查User-Agent：確認是否為瀏覽器訪問。解決：在request.get中加入headers設定]\n",
    "\n",
    "2. IP代理\n",
    "由於爬蟲常常需要多次爬取某些網站，網站之上根據訪問次數判斷是否為爬蟲，若次數很多時，將會封禁此時使用的IP，應對方法為尋找代理IP，若封禁某些IP， 則使用代理IP繼續爬取。\n",
    "實現步驟：\n",
    "\n",
    "1）。 調用urllib.request.ProxyHandler（proxies = None）； —類似的理解為Request對象\n",
    "2）。 調用opener—類似與urlopen，這個是定制的\n",
    "3）。 安裝開啟器\n",
    "4）。 代理IP的選擇 \n",
    "from  urllib import  request\n",
    "from urllib.error import URLError\n",
    "# url = 'https://www.whatismyip.com/'\n",
    "url = 'https://httpbin.org/get'\n",
    "proxy = {'https':'171.221.239.11:808', 'http':'218.14.115.211:3128'}\n",
    "user_agent = 'Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101 Firefox/45.0'\n",
    "# 1).调用urllib.request.ProxyHandler(proxies=None)；  --- 类似理解为Request对象\n",
    "proxy_support = request.ProxyHandler(proxy)\n",
    "# 2).调用Opener - -- 类似与urlopen， 这个是定制的\n",
    "opener = request.build_opener(proxy_support)\n",
    "# 伪装浏览器\n",
    "opener.addheaders = [('User-Agent',user_agent)]\n",
    "# 3).安装Opener\n",
    "request.install_opener(opener)\n",
    "# 4).代理IP的选择\n",
    "response = request.urlopen(url)\n",
    "content  = response.read().decode('utf-8')    #content即为爬取结果\n",
    "\n",
    "[訪問頻率限制：頻率太快會。解決：使用隨機休息秒數或在proxies中設定代理IP]\n",
    "\n",
    "3. 蜜罐技術：故意設置漏洞，一旦入侵就將之封鎖。解決：定向爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}