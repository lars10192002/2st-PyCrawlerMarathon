{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加速：多線程爬蟲\n",
    "\n",
    "\n",
    "\n",
    "* 了解知乎 API 使用方式與回傳內容\n",
    "* 撰寫程式存取 API 且添加標頭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://zhuanlan.zhihu.com/p/35808861\n",
    "#https://my.oschina.net/gain/blog/1794659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "* 找一個之前實作過的爬蟲改用多線程改寫，比較前後時間的差異。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 單線程爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_article(url):\n",
    "    response = requests.get(url, cookies={'over18': '1'})\n",
    "    \n",
    "    # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "    if response.status_code != 200:\n",
    "        print('Error - {} is not available to access'.format(url))\n",
    "        return\n",
    "    \n",
    "    # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # 取得文章內容主體\n",
    "    main_content = soup.find(id='main-content')\n",
    "    \n",
    "    # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "    metas = main_content.select('div.article-metaline')\n",
    "    author = ''\n",
    "    title = ''\n",
    "    date = ''\n",
    "    if metas:\n",
    "        if metas[0].select('span.article-meta-value')[0]:\n",
    "            author = metas[0].select('span.article-meta-value')[0].string\n",
    "        if metas[1].select('span.article-meta-value')[0]:\n",
    "            title = metas[1].select('span.article-meta-value')[0].string\n",
    "        if metas[2].select('span.article-meta-value')[0]:\n",
    "            date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "        # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "        #\n",
    "        # .extract() 方法可以參考官方文件\n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "        for m in metas:\n",
    "            m.extract()\n",
    "        for m in main_content.select('div.article-metaline-right'):\n",
    "            m.extract()\n",
    "    \n",
    "    # 取得留言區主體\n",
    "    pushes = main_content.find_all('div', class_='push')\n",
    "    for p in pushes:\n",
    "        p.extract()\n",
    "    \n",
    "    # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "    # 透過 regular expression 取得 IP\n",
    "    # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "    try:\n",
    "        ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "        ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "    except Exception as e:\n",
    "        ip = ''\n",
    "    \n",
    "    # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "    # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "    #\n",
    "    # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "    #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "    filtered = []\n",
    "    for v in main_content.stripped_strings:\n",
    "        # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "        if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "            filtered.append(v)\n",
    "\n",
    "    # 定義一些特殊符號與全形符號的過濾器\n",
    "    expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "    for i in range(len(filtered)):\n",
    "        filtered[i] = re.sub(expr, '', filtered[i])\n",
    "    \n",
    "    # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "    filtered = [i for i in filtered if i]\n",
    "    content = ' '.join(filtered)\n",
    "    \n",
    "    # 處理留言區\n",
    "    # p 計算推文數量\n",
    "    # b 計算噓文數量\n",
    "    # n 計算箭頭數量\n",
    "    p, b, n = 0, 0, 0\n",
    "    messages = []\n",
    "    for push in pushes:\n",
    "        # 假如留言段落沒有 push-tag 就跳過\n",
    "        if not push.find('span', 'push-tag'):\n",
    "            continue\n",
    "        \n",
    "        # 過濾額外空白與換行符號\n",
    "        # push_tag 判斷是推文, 箭頭還是噓文\n",
    "        # push_userid 判斷留言的人是誰\n",
    "        # push_content 判斷留言內容\n",
    "        # push_ipdatetime 判斷留言日期時間\n",
    "        push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "        push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "        push_content = push.find('span', 'push-content').strings\n",
    "        push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "        push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 整理打包留言的資訊, 並統計推噓文數量\n",
    "        messages.append({\n",
    "            'push_tag': push_tag,\n",
    "            'push_userid': push_userid,\n",
    "            'push_content': push_content,\n",
    "            'push_ipdatetime': push_ipdatetime})\n",
    "        if push_tag == u'推':\n",
    "            p += 1\n",
    "        elif push_tag == u'噓':\n",
    "            b += 1\n",
    "        else:\n",
    "            n += 1\n",
    "    \n",
    "    # 統計推噓文\n",
    "    # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "    # all 為總共留言數量 \n",
    "    message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "    \n",
    "    # 整理文章資訊\n",
    "    data = {\n",
    "        'url': url,\n",
    "        'article_author': author,\n",
    "        'article_title': title,\n",
    "        'article_date': date,\n",
    "        'article_content': content,\n",
    "        'ip': ip,\n",
    "        'message_count': message_count,\n",
    "        'messages': messages\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parse [政治] 怨預購口罩兩次都沒抽到 林珍羽道歉:誤 - https://www.ptt.cc/bbs/Gossiping/M.1586136514.A.78D.html\nParse [政治] 空拍曝光！韓當市長後 高雄這地方居然變 - https://www.ptt.cc/bbs/Gossiping/M.1586136538.A.68B.html\nParse Re: [問卦] 白糖粿好吃嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136597.A.FCD.html\nParse [問卦] 會看胡子影片的都是怎麼樣的人 - https://www.ptt.cc/bbs/Gossiping/M.1586136603.A.5A1.html\nParse [問卦] 有沒有王勇平的八卦？ - https://www.ptt.cc/bbs/Gossiping/M.1586136679.A.FE0.html\nParse [爆卦] UTC 2020/04/05 各國新增病例與死亡排行 - https://www.ptt.cc/bbs/Gossiping/M.1586136682.A.D2B.html\nParse Re: [政治] 民眾黨2022布局 拚遍地開花 - https://www.ptt.cc/bbs/Gossiping/M.1586136692.A.FD6.html\nParse [問卦] 講到齋藤一會想到蟑螂鬚是不是表示老了？ - https://www.ptt.cc/bbs/Gossiping/M.1586136698.A.234.html\nParse Re: [問卦] 中國都沒有軍人集體感染嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136699.A.685.html\nParse [問卦] COACH 這牌子的價格到底？ - https://www.ptt.cc/bbs/Gossiping/M.1586136785.A.736.html\nParse Re: [問卦] 阿中為什麼不宣布禁出遊 - https://www.ptt.cc/bbs/Gossiping/M.1586136801.A.FB0.html\nParse [問卦] 納智捷倒掉會影響很多人嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136847.A.8AC.html\nParse [問卦] 日本防疫在想什麼 - https://www.ptt.cc/bbs/Gossiping/M.1586136896.A.34A.html\nParse [新聞] 紐約佛心房東心疼房客 18棟公寓租金全免 - https://www.ptt.cc/bbs/Gossiping/M.1586136923.A.86A.html\nParse [問卦] 帶口罩鼻孔很癢怎辦 - https://www.ptt.cc/bbs/Gossiping/M.1586136960.A.510.html\nParse [爆卦] 疾管署0406記者會直播（10：00開始） - https://www.ptt.cc/bbs/Gossiping/M.1586136994.A.655.html\nParse Re: [政治] 怨預購口罩兩次都沒抽到 林珍羽道歉:誤 - https://www.ptt.cc/bbs/Gossiping/M.1586137188.A.854.html\nParse [問卦] 桌上多了一杯咖啡？ - https://www.ptt.cc/bbs/Gossiping/M.1586137188.A.176.html\nParse [問卦] 古代人都亂吃怎麼沒有因此造成瘟疫？ - https://www.ptt.cc/bbs/Gossiping/M.1586137190.A.CAB.html\nParse [新聞]美衛生總監:這周將是\"珍珠港\"和\"911\"時刻  - https://www.ptt.cc/bbs/Gossiping/M.1586137223.A.805.html\nReach the last article\n共用時： 5.503347158432007\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "\n",
    "stime = time.time()\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.findChildren('div', recursive=False):\n",
    "    class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "    \n",
    "    # 遇到分隔線要處理的情況\n",
    "    if class_name and 'r-list-sep' in class_name:\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    \n",
    "    # 遇到目標文章\n",
    "    if class_name and 'r-ent' in class_name:\n",
    "        div_title = div.find('div', class_='title')\n",
    "        a_title = div_title.find('a', href=True)\n",
    "        if a_title:\n",
    "            article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "        else:\n",
    "            article_URL = None\n",
    "            a_title = '<a>本文已刪除</a>'\n",
    "        article_title = a_title.text\n",
    "        print('Parse {} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 呼叫上面寫好的 function 來對文章進行爬蟲\n",
    "        if article_URL:\n",
    "            parse_data = crawl_article(article_URL) # 返回單一文章資訊的字典\n",
    "        \n",
    "        # 將爬完的資料儲存\n",
    "        all_data.append(parse_data)\n",
    "        \n",
    "etime = time.time()\n",
    "print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 多線程爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parse [政治] 怨預購口罩兩次都沒抽到 林珍羽道歉:誤 - https://www.ptt.cc/bbs/Gossiping/M.1586136514.A.78D.html\nParse [政治] 空拍曝光！韓當市長後 高雄這地方居然變 - https://www.ptt.cc/bbs/Gossiping/M.1586136538.A.68B.html\nParse Re: [問卦] 白糖粿好吃嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136597.A.FCD.html\nParse [問卦] 會看胡子影片的都是怎麼樣的人 - https://www.ptt.cc/bbs/Gossiping/M.1586136603.A.5A1.html\nParse [問卦] 有沒有王勇平的八卦？ - https://www.ptt.cc/bbs/Gossiping/M.1586136679.A.FE0.html\nParse [爆卦] UTC 2020/04/05 各國新增病例與死亡排行 - https://www.ptt.cc/bbs/Gossiping/M.1586136682.A.D2B.html\nParse Re: [政治] 民眾黨2022布局 拚遍地開花 - https://www.ptt.cc/bbs/Gossiping/M.1586136692.A.FD6.html\nParse [問卦] 講到齋藤一會想到蟑螂鬚是不是表示老了？ - https://www.ptt.cc/bbs/Gossiping/M.1586136698.A.234.html\nParse Re: [問卦] 中國都沒有軍人集體感染嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136699.A.685.html\nParse [問卦] COACH 這牌子的價格到底？ - https://www.ptt.cc/bbs/Gossiping/M.1586136785.A.736.html\nParse Re: [問卦] 阿中為什麼不宣布禁出遊 - https://www.ptt.cc/bbs/Gossiping/M.1586136801.A.FB0.html\nParse [問卦] 納智捷倒掉會影響很多人嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136847.A.8AC.html\nParse [問卦] 日本防疫在想什麼 - https://www.ptt.cc/bbs/Gossiping/M.1586136896.A.34A.html\nParse [新聞] 紐約佛心房東心疼房客 18棟公寓租金全免 - https://www.ptt.cc/bbs/Gossiping/M.1586136923.A.86A.html\nParse [問卦] 帶口罩鼻孔很癢怎辦 - https://www.ptt.cc/bbs/Gossiping/M.1586136960.A.510.html\nParse [爆卦] 疾管署0406記者會直播（10：00開始） - https://www.ptt.cc/bbs/Gossiping/M.1586136994.A.655.html\nParse Re: [政治] 怨預購口罩兩次都沒抽到 林珍羽道歉:誤 - https://www.ptt.cc/bbs/Gossiping/M.1586137188.A.854.html\nParse [問卦] 桌上多了一杯咖啡？ - https://www.ptt.cc/bbs/Gossiping/M.1586137188.A.176.html\nParse [問卦] 古代人都亂吃怎麼沒有因此造成瘟疫？ - https://www.ptt.cc/bbs/Gossiping/M.1586137190.A.CAB.html\nParse [新聞]美衛生總監:這周將是\"珍珠港\"和\"911\"時刻  - https://www.ptt.cc/bbs/Gossiping/M.1586137223.A.805.html\nReach the last article\n共用時： 0.724128007888794\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "all_url = []\n",
    "\n",
    "stime = time.time()\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.findChildren('div', recursive=False):\n",
    "    class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "    \n",
    "    # 遇到分隔線要處理的情況\n",
    "    if class_name and 'r-list-sep' in class_name:\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    \n",
    "    # 遇到目標文章\n",
    "    if class_name and 'r-ent' in class_name:\n",
    "        div_title = div.find('div', class_='title')\n",
    "        a_title = div_title.find('a', href=True)\n",
    "        if a_title:\n",
    "            article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "        else:\n",
    "            article_URL = None\n",
    "            a_title = '<a>本文已刪除</a>'\n",
    "        article_title = a_title.text\n",
    "        print('Parse {} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 把文章連結存在list\n",
    "        if article_URL:\n",
    "            all_url.append(article_URL)\n",
    "\n",
    "# 從這裡丟給子執行緒工作            \n",
    "# 建立 n 個子執行緒，分別去抓文章內容\n",
    "threads = []\n",
    "for i in range(len(all_url)):\n",
    "    threads.append(threading.Thread(target = crawl_article, args = (all_url[i],)))\n",
    "    threads[i].start()\n",
    "\n",
    "# 主執行緒繼續執行自己的工作\n",
    "# ...\n",
    "\n",
    "# 等待所有子執行緒結束\n",
    "for i in range(len(all_url)):\n",
    "    threads[i].join()\n",
    "\n",
    "        \n",
    "etime = time.time()\n",
    "print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 物件導向寫法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTask(threading.Thread):\n",
    "    def __init__(self, task_name):\n",
    "        super(MyTask, self).__init__()\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Get task: {}\\n\".format(self.task_name))\n",
    "        time.sleep(1)\n",
    "        print(\"Finish task: {}\\n\".format(self.task_name))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = [1,2,3,4,5,6,7,8,9,10]\n",
    "    tasks = []\n",
    "    for i in range(0, 10):\n",
    "        # 建立 task\n",
    "        tasks.append(MyTask(\"task_{}\".format(data[i])))\n",
    "    for t in tasks:\n",
    "        # 開始執行 task\n",
    "        t.start()\n",
    "\n",
    "    for t in tasks:\n",
    "        # 等待 task 執行完畢\n",
    "        # 完畢前會阻塞住主執行緒\n",
    "        t.join()\n",
    "    print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "改寫:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parse [政治] 怨預購口罩兩次都沒抽到 林珍羽道歉:誤 - https://www.ptt.cc/bbs/Gossiping/M.1586136514.A.78D.html\nParse [政治] 空拍曝光！韓當市長後 高雄這地方居然變 - https://www.ptt.cc/bbs/Gossiping/M.1586136538.A.68B.html\nParse Re: [問卦] 白糖粿好吃嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136597.A.FCD.html\nParse [問卦] 會看胡子影片的都是怎麼樣的人 - https://www.ptt.cc/bbs/Gossiping/M.1586136603.A.5A1.html\nParse [問卦] 有沒有王勇平的八卦？ - https://www.ptt.cc/bbs/Gossiping/M.1586136679.A.FE0.html\nParse [爆卦] UTC 2020/04/05 各國新增病例與死亡排行 - https://www.ptt.cc/bbs/Gossiping/M.1586136682.A.D2B.html\nParse Re: [政治] 民眾黨2022布局 拚遍地開花 - https://www.ptt.cc/bbs/Gossiping/M.1586136692.A.FD6.html\nParse [問卦] 講到齋藤一會想到蟑螂鬚是不是表示老了？ - https://www.ptt.cc/bbs/Gossiping/M.1586136698.A.234.html\nParse Re: [問卦] 中國都沒有軍人集體感染嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136699.A.685.html\nReach the last article\n共9個連結\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136514.A.78D.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136538.A.68B.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136597.A.FCD.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136603.A.5A1.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136679.A.FE0.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136682.A.D2B.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136692.A.FD6.html\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136698.A.234.html\n\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136699.A.685.html\n\n共用時： 0.3496057987213135\n"
    }
   ],
   "source": [
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.url = url\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        print(\"Get子執行緒: {}\\n\".format(self.url))\n",
    "\n",
    "        response = requests.get(self.url, cookies={'over18': '1'})\n",
    "\n",
    "        # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "        if response.status_code != 200:\n",
    "            print('Error - {} is not available to access'.format(self.url))\n",
    "            return\n",
    "\n",
    "        # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "        soup = BeautifulSoup(response.text)\n",
    "\n",
    "        # 取得文章內容主體\n",
    "        main_content = soup.find(id='main-content')\n",
    "\n",
    "        # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "        metas = main_content.select('div.article-metaline') #list\n",
    "        author = ''\n",
    "        title = ''\n",
    "        date = ''\n",
    "        if metas:\n",
    "            if metas[0].select('span.article-meta-value')[0]:\n",
    "                author = metas[0].select('span.article-meta-value')[0].string\n",
    "            if metas[1].select('span.article-meta-value')[0]:\n",
    "                title = metas[1].select('span.article-meta-value')[0].string\n",
    "            if metas[2].select('span.article-meta-value')[0]:\n",
    "                date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "            # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "            #\n",
    "            # .extract() 方法可以參考官方文件\n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "            for m in metas:\n",
    "                m.extract()\n",
    "            for m in main_content.select('div.article-metaline-right'):\n",
    "                m.extract()\n",
    "\n",
    "        # 取得留言區主體\n",
    "        pushes = main_content.find_all('div', class_='push')\n",
    "        for p in pushes:\n",
    "            p.extract()\n",
    "\n",
    "        # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "        # 透過 regular expression 取得 IP\n",
    "        # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "        try:\n",
    "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "        except Exception as e:\n",
    "            ip = ''\n",
    "\n",
    "        # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "        # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "        #\n",
    "        # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "        filtered = []\n",
    "        for v in main_content.stripped_strings:\n",
    "            # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                filtered.append(v)\n",
    "\n",
    "        # 定義一些特殊符號與全形符號的過濾器\n",
    "        expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "        for i in range(len(filtered)):\n",
    "            filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "        # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "        filtered = [i for i in filtered if i]\n",
    "        content = ' '.join(filtered)\n",
    "\n",
    "        # 處理留言區\n",
    "        # p 計算推文數量\n",
    "        # b 計算噓文數量\n",
    "        # n 計算箭頭數量\n",
    "        p, b, n = 0, 0, 0\n",
    "        messages = []\n",
    "        for push in pushes:\n",
    "            # 假如留言段落沒有 push-tag 就跳過\n",
    "            if not push.find('span', 'push-tag'):\n",
    "                continue\n",
    "\n",
    "            # 過濾額外空白與換行符號\n",
    "            # push_tag 判斷是推文, 箭頭還是噓文\n",
    "            # push_userid 判斷留言的人是誰\n",
    "            # push_content 判斷留言內容\n",
    "            # push_ipdatetime 判斷留言日期時間\n",
    "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "            push_content = push.find('span', 'push-content').strings\n",
    "            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "            # 整理打包留言的資訊, 並統計推噓文數量\n",
    "            messages.append({\n",
    "                'push_tag': push_tag,\n",
    "                'push_userid': push_userid,\n",
    "                'push_content': push_content,\n",
    "                'push_ipdatetime': push_ipdatetime})\n",
    "            if push_tag == u'推':\n",
    "                p += 1\n",
    "            elif push_tag == u'噓':\n",
    "                b += 1\n",
    "            else:\n",
    "                n += 1\n",
    "\n",
    "        # 統計推噓文\n",
    "        # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "        # all 為總共留言數量 \n",
    "        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "        # 整理文章資訊\n",
    "        data = {\n",
    "            'url': self.url,\n",
    "            'article_author': author,\n",
    "            'article_title': title,\n",
    "            'article_date': date,\n",
    "            'article_content': content,\n",
    "            'ip': ip,\n",
    "            'message_count': message_count,\n",
    "            'messages': messages\n",
    "        }\n",
    "        return data\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    all_url = []\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "            article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                all_url.append(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(len(all_url)))\n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(len(all_url)):\n",
    "        threads.append(Crawl_Article(all_url[i]))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "使用佇列 Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parse [政治] 怨預購口罩兩次都沒抽到 林珍羽道歉:誤 - https://www.ptt.cc/bbs/Gossiping/M.1586136514.A.78D.html\nParse [政治] 空拍曝光！韓當市長後 高雄這地方居然變 - https://www.ptt.cc/bbs/Gossiping/M.1586136538.A.68B.html\nParse Re: [問卦] 白糖粿好吃嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136597.A.FCD.html\nParse [問卦] 會看胡子影片的都是怎麼樣的人 - https://www.ptt.cc/bbs/Gossiping/M.1586136603.A.5A1.html\nParse [問卦] 有沒有王勇平的八卦？ - https://www.ptt.cc/bbs/Gossiping/M.1586136679.A.FE0.html\nParse [爆卦] UTC 2020/04/05 各國新增病例與死亡排行 - https://www.ptt.cc/bbs/Gossiping/M.1586136682.A.D2B.html\nParse Re: [政治] 民眾黨2022布局 拚遍地開花 - https://www.ptt.cc/bbs/Gossiping/M.1586136692.A.FD6.html\nParse [問卦] 講到齋藤一會想到蟑螂鬚是不是表示老了？ - https://www.ptt.cc/bbs/Gossiping/M.1586136698.A.234.html\nParse Re: [問卦] 中國都沒有軍人集體感染嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586136699.A.685.html\nReach the last article\n共9個連結\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136514.A.78D.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136538.A.68B.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136597.A.FCD.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136603.A.5A1.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136679.A.FE0.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136682.A.D2B.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136692.A.FD6.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136698.A.234.html\n\nGet子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1586136699.A.685.html\n\n共用時： 0.3346219062805176\n"
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.queue = queue\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        # 當 queue 裡面有資料再執行\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "            print(\"Get子執行緒: {}\\n\".format(url))\n",
    "\n",
    "            response = requests.get(url, cookies={'over18': '1'})\n",
    "\n",
    "            # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "            if response.status_code != 200:\n",
    "                print('Error - {} is not available to access'.format(url))\n",
    "                return\n",
    "\n",
    "            # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            # 取得文章內容主體\n",
    "            main_content = soup.find(id='main-content')\n",
    "\n",
    "            # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "            metas = main_content.select('div.article-metaline') #list\n",
    "            author = ''\n",
    "            title = ''\n",
    "            date = ''\n",
    "            if metas:\n",
    "                if metas[0].select('span.article-meta-value')[0]:\n",
    "                    author = metas[0].select('span.article-meta-value')[0].string\n",
    "                if metas[1].select('span.article-meta-value')[0]:\n",
    "                    title = metas[1].select('span.article-meta-value')[0].string\n",
    "                if metas[2].select('span.article-meta-value')[0]:\n",
    "                    date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "                # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "                #\n",
    "                # .extract() 方法可以參考官方文件\n",
    "                #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "                for m in metas:\n",
    "                    m.extract()\n",
    "                for m in main_content.select('div.article-metaline-right'):\n",
    "                    m.extract()\n",
    "\n",
    "            # 取得留言區主體\n",
    "            pushes = main_content.find_all('div', class_='push')\n",
    "            for p in pushes:\n",
    "                p.extract()\n",
    "\n",
    "            # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "            # 透過 regular expression 取得 IP\n",
    "            # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "            try:\n",
    "                ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "                ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "            except Exception as e:\n",
    "                ip = ''\n",
    "\n",
    "            # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "            # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "            #\n",
    "            # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "            filtered = []\n",
    "            for v in main_content.stripped_strings:\n",
    "                # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "                if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                    filtered.append(v)\n",
    "\n",
    "            # 定義一些特殊符號與全形符號的過濾器\n",
    "            expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "            for i in range(len(filtered)):\n",
    "                filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "            # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "            filtered = [i for i in filtered if i]\n",
    "            content = ' '.join(filtered)\n",
    "\n",
    "            # 處理留言區\n",
    "            # p 計算推文數量\n",
    "            # b 計算噓文數量\n",
    "            # n 計算箭頭數量\n",
    "            p, b, n = 0, 0, 0\n",
    "            messages = []\n",
    "            for push in pushes:\n",
    "                # 假如留言段落沒有 push-tag 就跳過\n",
    "                if not push.find('span', 'push-tag'):\n",
    "                    continue\n",
    "\n",
    "                # 過濾額外空白與換行符號\n",
    "                # push_tag 判斷是推文, 箭頭還是噓文\n",
    "                # push_userid 判斷留言的人是誰\n",
    "                # push_content 判斷留言內容\n",
    "                # push_ipdatetime 判斷留言日期時間\n",
    "                push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "                push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "                # 整理打包留言的資訊, 並統計推噓文數量\n",
    "                messages.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "                if push_tag == u'推':\n",
    "                    p += 1\n",
    "                elif push_tag == u'噓':\n",
    "                    b += 1\n",
    "                else:\n",
    "                    n += 1\n",
    "\n",
    "            # 統計推噓文\n",
    "            # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "            # all 為總共留言數量 \n",
    "            message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "            # 整理文章資訊\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'article_author': author,\n",
    "                'article_title': title,\n",
    "                'article_date': date,\n",
    "                'article_content': content,\n",
    "                'ip': ip,\n",
    "                'message_count': message_count,\n",
    "                'messages': messages\n",
    "            }\n",
    "            return data\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    \n",
    "    Q_url = Queue()\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "            article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                Q_url.put(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(Q_url.qsize()))\n",
    "    \n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(Q_url.qsize()):\n",
    "        threads.append(Crawl_Article(Q_url))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 使用lock: \n",
    "被 Lock 的 acquire 與 release 包起來的這段程式碼不會被兩個執行緒同時執行。\n",
    "用來寫入檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "子執行緒 100 取得lock\n子執行緒 100: 寫入檔案 Url 1\n子執行緒 100 釋放lock\n子執行緒 200 取得lock\n子執行緒 200: 寫入檔案 Url 2\n子執行緒 200 釋放lock\n子執行緒 100 取得lock\n子執行緒 100: 寫入檔案 Url 3\n子執行緒 100 釋放lock\n子執行緒 200 取得lock\n子執行緒 200: 寫入檔案 Url 4\n子執行緒 200 釋放lock\n子執行緒 100 取得lock\n子執行緒 100: 寫入檔案 Url 5\n子執行緒 100 釋放lock\nDone.\n"
    }
   ],
   "source": [
    "class Worker(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue, num, lock):\n",
    "        \n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.num = num\n",
    "        self.lock = lock\n",
    "\n",
    "    def run(self):\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "\n",
    "            # 取得 lock\n",
    "            lock.acquire()\n",
    "            print(\"子執行緒 %d 取得lock\" % self.num)\n",
    "\n",
    "            # 不能讓多個執行緒同時進的工作\n",
    "            print(\"子執行緒 %d: 寫入檔案 %s\" % (self.num, url))\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 釋放 lock\n",
    "            print(\"子執行緒 %d 釋放lock\" % self.num)\n",
    "            self.lock.release()\n",
    "            \n",
    "#建立一個佇列\n",
    "my_queue = Queue()\n",
    "\n",
    "#假裝放五個URL進去queue\n",
    "for i in range(1,6):\n",
    "    my_queue.put(\"Url %d\" % i)\n",
    "\n",
    "# 建立 lock\n",
    "lock = threading.Lock()\n",
    "\n",
    "#建立2個子執行緒，傳入queue和一個參數和lock\n",
    "my_worker1 = Worker(my_queue, 100, lock)\n",
    "my_worker2 = Worker(my_queue, 200, lock)\n",
    "\n",
    "my_worker1.start()\n",
    "my_worker2.start()\n",
    "\n",
    "my_worker1.join()\n",
    "my_worker2.join()\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "改寫：加入能寫入檔案的lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parse [新聞] 不爽鄰居在門前抽菸持鋁棒打爆頭 賠500萬 - https://www.ptt.cc/bbs/Gossiping/M.1586138557.A.C18.html\nParse Re: [問卦] 還有現役大學生 死守PTT嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586138582.A.BBA.html\nParse [問卦] 不被中國承認的發明有哪些啊？ - https://www.ptt.cc/bbs/Gossiping/M.1586138728.A.04C.html\nParse [問卦] 承認有去景點的富邦員工年終會怎樣? - https://www.ptt.cc/bbs/Gossiping/M.1586138742.A.422.html\nParse Re: [新聞] 國旅「染疫警覺性」視同出國！就醫需一併 - https://www.ptt.cc/bbs/Gossiping/M.1586138755.A.7BB.html\nParse Re: [新聞] 國旅「染疫警覺性」視同出國！就醫需一併 - https://www.ptt.cc/bbs/Gossiping/M.1586138791.A.E77.html\nParse [爆卦] 確診+10  本土1  境外移入9 - https://www.ptt.cc/bbs/Gossiping/M.1586138792.A.55A.html\nParse <a>本文已刪除</a> - None\nParse [問卦] 11個警報景點讓一堆人居家辦公的卦?! - https://www.ptt.cc/bbs/Gossiping/M.1586138888.A.A4E.html\nParse [新聞] 停車巧遇檢舉達人 車主跑喊「追你到天亮 - https://www.ptt.cc/bbs/Gossiping/M.1586138889.A.D90.html\nParse [問卦] 為啥泡麵剩韓國的或是台酒滿漢一度讚？ - https://www.ptt.cc/bbs/Gossiping/M.1586138903.A.20D.html\nParse [問卦] 有學校上課要戴口罩了嗎 - https://www.ptt.cc/bbs/Gossiping/M.1586138908.A.835.html\nParse Re: [問卦] 還有現役大學生 死守PTT嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1586139014.A.49E.html\nReach the last article\n共12個連結\n共用時： 0.040361881256103516\n"
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue, lock):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.lock = lock\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        # 當 queue 裡面有資料再執行\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "            #print(\"Get子執行緒: {}\\n\".format(url))\n",
    "\n",
    "            response = requests.get(url, cookies={'over18': '1'})\n",
    "\n",
    "            # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "            if response.status_code != 200:\n",
    "                print('Error - {} is not available to access'.format(url))\n",
    "                return\n",
    "\n",
    "            # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            # 取得文章內容主體\n",
    "            main_content = soup.find(id='main-content')\n",
    "\n",
    "            # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "            metas = main_content.select('div.article-metaline') #list\n",
    "            author = ''\n",
    "            title = ''\n",
    "            date = ''\n",
    "            if metas:\n",
    "                if metas[0].select('span.article-meta-value')[0]:\n",
    "                    author = metas[0].select('span.article-meta-value')[0].string\n",
    "                if metas[1].select('span.article-meta-value')[0]:\n",
    "                    title = metas[1].select('span.article-meta-value')[0].string\n",
    "                if metas[2].select('span.article-meta-value')[0]:\n",
    "                    date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "                # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "                #\n",
    "                # .extract() 方法可以參考官方文件\n",
    "                #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "                for m in metas:\n",
    "                    m.extract()\n",
    "                for m in main_content.select('div.article-metaline-right'):\n",
    "                    m.extract()\n",
    "\n",
    "            # 取得留言區主體\n",
    "            pushes = main_content.find_all('div', class_='push')\n",
    "            for p in pushes:\n",
    "                p.extract()\n",
    "\n",
    "            # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "            # 透過 regular expression 取得 IP\n",
    "            # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "            try:\n",
    "                ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "                ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "            except Exception as e:\n",
    "                ip = ''\n",
    "\n",
    "            # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "            # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "            #\n",
    "            # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "            filtered = []\n",
    "            for v in main_content.stripped_strings:\n",
    "                # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "                if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                    filtered.append(v)\n",
    "\n",
    "            # 定義一些特殊符號與全形符號的過濾器\n",
    "            expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "            for i in range(len(filtered)):\n",
    "                filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "            # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "            filtered = [i for i in filtered if i]\n",
    "            content = ' '.join(filtered)\n",
    "\n",
    "            # 處理留言區\n",
    "            # p 計算推文數量\n",
    "            # b 計算噓文數量\n",
    "            # n 計算箭頭數量\n",
    "            p, b, n = 0, 0, 0\n",
    "            messages = []\n",
    "            for push in pushes:\n",
    "                # 假如留言段落沒有 push-tag 就跳過\n",
    "                if not push.find('span', 'push-tag'):\n",
    "                    continue\n",
    "\n",
    "                # 過濾額外空白與換行符號\n",
    "                # push_tag 判斷是推文, 箭頭還是噓文\n",
    "                # push_userid 判斷留言的人是誰\n",
    "                # push_content 判斷留言內容\n",
    "                # push_ipdatetime 判斷留言日期時間\n",
    "                push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "                push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "                # 整理打包留言的資訊, 並統計推噓文數量\n",
    "                messages.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "                if push_tag == u'推':\n",
    "                    p += 1\n",
    "                elif push_tag == u'噓':\n",
    "                    b += 1\n",
    "                else:\n",
    "                    n += 1\n",
    "\n",
    "            # 統計推噓文\n",
    "            # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "            # all 為總共留言數量 \n",
    "            message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "            # 整理文章資訊\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'article_author': author,\n",
    "                'article_title': title,\n",
    "                'article_date': date,\n",
    "                'article_content': content,\n",
    "                'ip': ip,\n",
    "                'message_count': message_count,\n",
    "                'messages': messages\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # 寫入檔案:單一文章內容\n",
    "            \n",
    "            # 取得 lock\n",
    "            lock.acquire()\n",
    "            #print(\"%s 取得lock\" % url[32:51])\n",
    "\n",
    "            # 不能讓多個執行緒同時進的工作 : 將爬完的資訊存成 json 檔案\n",
    "            #print(\"寫入檔案\")\n",
    "            with open('../Data/PTT_Article.json', 'a+', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "                f.write(\",\")\n",
    "\n",
    "            # 釋放 lock\n",
    "            #print(\"%s 釋放lock\" % url[32:51])\n",
    "            self.lock.release()\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    \n",
    "    Q_url = Queue()\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            \n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "                article_title = a_title.text\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "                article_title = a_title\n",
    "                \n",
    "            #article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                Q_url.put(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(Q_url.qsize()))\n",
    "    \n",
    "    # 建立 lock\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(Q_url.qsize()):\n",
    "        threads.append(Crawl_Article(Q_url, lock))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    # for i in range(len(all_url)):\n",
    "    #     threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}